[[https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print12.pdf][The Elements of Statistical Learning]]

* Introduction
  In a typical scenario, we have an outcome measurement, usually quantitive or categorical
  that we wish to predict based on a set of features. We have a training set of data,
  in which we observe the outcome and feature measurements for a set of objects.
* Overview of Supervised Learning
** Introduction
   Inputs = Predictors = Independent Variables = Features
   Outputs = Responses = Dependent Variables
** Variable Types and Terminology
   A variable type is /ordered categorical/, such as /small/, /medium/ and /large/, where
   there is an ordering between the values, but no metric notion is appropriate.
   
   Qualitative variables are typically represented numerically by codes.
   The easiest case is when there are only 2 classes or categories.
   For reasons that will become apparent, such numeric codes are sometimes referred
   to as /targets/.

   When there are more than two categories, several alternatives are available.
   The most useful and commonly used coding is via /dummy variables/.
   We will typically denote an input variable by the symbol X.
   Quantitative outputs will be denoted by Y and qualitative outputs by G.

   For the moment we can loosely state the learning task as follows:
   given the value of an input vector X, make a good prediction of the output Y,
   denoted by ^Y (pronunced "y-hat"). If Y takes values in R, then so should ^Y.
** Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors
   The linear model makes huge assumptions about structure and yields stable but possibly
   inaccurate predictions. The method of k-nearest neighbors makes very mild structural
   assumptions: its predictions are often accurate but can be unstable.
*** Linear Models and Least Squares
    Given a vector of inputs X^t = (X1, X2, ..., Xp), we predict the output Y.
    Often it's convenient to write the linear model in vector form as an inner product.
*** Nearest-Neighbor Methods
    Nearest-neighbor methods use those observations in the training set T closest
    in input space to x to form ^Y.
    For k-nearest-neighbor fits, the error on the training data should be
    approximately an increasing function of k, and will always be 0 for k = 1.
*** From Least Squares to Nearest Neighbors
    The linear decision boundry from least squares is very smooth, and apparently
    stable to fit. It has low variance and potentially high bias.
    On the other hand, the k-nearest-neighbor procedures do not appear to rely
    on any stringet assumptions - high variance and low bias.
** Statistical Decision Theory
   Least squares assumes f(x) is well approximated by a globally linear function.
   k-nearest neighbors assumes f(x) is well approximated by a locally constant function.
** Local Methods in High Dimensions
   It would seem that with a reasonably large set of training data, we could always
   approximate the theoretically optimal conditional expectation by k-nearest-neighbor
   averaging, since we should be able to find a fairly large neighborhood of
   observations close to any x and average them. This approach and our intuition breaks
   down in high dimensions, and the phenomenon is commonly referred to as the curse
   of dimensionality.
** Statistical Models, Supervised Learning and Function Approximation
*** A Statistical Model for the Join Distribution Pr(X,Y)
*** Supervised Learning
*** Function Approximation
** Structured Regression Models
*** Difficulty of the Problem
** Classes of Restricted Estimators
*** Roughness Penalty and Bayesian Methods
*** Basis Functions and Dictionary Methods 
** Model Selection and the Bias-Variance Tradeoff
* Linear Methods for Regression
** Introduction
   For prediction purposes they can sometimes outperform fancier nonlinear models,
   especially in situations with small numbers of training cases, low signal-to-noise
   ratio or sparse data. Finally, linear methods can be applied to transformations of
   the inputs and this considerably expands their scope.
** Linear Regression MOdels and Least Squares
