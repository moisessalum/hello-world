[[https://jakevdp.github.io/PythonDataScienceHandbook/][Python Data Science Handbook]]

* What is Machine Learning?
** Categories of Machine Learning
*** Supervised Learning
    Modeling the relationship between measured features of data ans dome label associated with the data;
    once this model is determined, it can be used to apply labels to new, unknown data.
**** Classification
     Discrete categories.
**** Regression
     Continuous quantities.
*** Unsupervised Learning
    Modeling the features of a dataset without reference to any label.
    Some examples:
**** Clustering
     Identify distinct groups of data.
**** Dimensionality reduction
     Search for more succinct representations of the data.

* Introducing Scikit-Learn
** Data Representation in Scikit-Learn
*** Data as table
**** Features matrix
     Matrix with [n_samples, n_features] (called x).
**** Target array
     Label or target array (called y). Usually one dimensional with length n_samples.
** Scikit-Learn's Estimator API
*** Consistency
*** Inspection
*** Limited Object Hierarchy
*** Composition
*** Sensible Defaults
** Basics of the API
*** Choose a class of model by importing the appropriate estimator class from Scikit-Learn.
*** Choose model hyperparameters by instantiating this class with desired values.
*** Arrange data into a features matrix and target vector following the discussion above.
*** Fit the model to your data by calling the "fit()" method of the model instance.
*** Apply the model to new data:
**** For supervised learning, often we predict lables for the unknown data using the "predict()" method.
**** For unsupervised learning, we often transform or infer properties of the data using the "transform()" or "predict()" method.
** Supervised learning example: Simple linear regression
   Fitting a line to (x,y) data. (To edit src C-c ')

   #+BEGIN_SRC python
     import matplotlib.pyplot as plt
     import numpy as np
     from sklearn.linear_model import LinearRegression

     # Generate data
     rng = np.random.RandomState(42)
     x = 10 * rng.rand(50)
     y = 2 * x - 1 + rng.randn(50)
     plt.scatter(x, y)

     # Choose a class of model (LinearRegression)
     # Choose model hyperparameters
     model = LinearRegression(fit_intercept=True)
     # Arrange data into a features matrix and target vector
     X = x[:, np.newaxis]
     print(X.shape)
     # Fit the model to your data
     model.fit(X, y)
     print(model.coef_)
     print(model.intercept_)
     # Predict labels for unknown data
     xfit = np.linspace(-1, 11)
     Xfit = xfit[:, np.newaxis]
     yfit = model.predict(Xfit)
   #+END_SRC
** Supervised learning example: Iris classification
   We will use Gaussian naive Bayes (GnB).
   Fast with no hyperparameters to choose, GnB is often a good model to use as a baseline classification.

   #+BEGIN_SRC python
     from sklearn.cross_validation import train_test_split
     from sklearn.naive_bayes import GaussianNB
     from sklearn.metrics import accuarcy_score

     # Generate training set and testing set
     Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)
     # Follow Basics of the API steps
     model = GaussianNB()
     model.fit(Xtrain, ytrain)
     y_model = model.predict(Xtest)
     print(accuarcy_score(ytest, y_model))
   #+END_SRC
