[[https://jakevdp.github.io/PythonDataScienceHandbook/][Python Data Science Handbook]]

* What is Machine Learning?
** [#B] Categories of Machine Learning
*** Supervised Learning
    Modeling the relationship between measured features of data ans dome label associated with the data;
    once this model is determined, it can be used to apply labels to new, unknown data.
**** Classification
     Discrete categories.
**** Regression
     Continuous quantities.
*** Unsupervised Learning
    Modeling the features of a dataset without reference to any label.
    Some examples:
**** Clustering
     Identify distinct groups of data.
**** Dimensionality reduction
     Search for more succinct representations of the data.

* Introducing Scikit-Learn
** Data Representation in Scikit-Learn
*** Data as table
**** Features matrix
     Matrix with [n_samples, n_features] (called x).
**** Target array
     Label or target array (called y). Usually one dimensional with length n_samples.
** Scikit-Learn's Estimator API
*** Consistency
*** Inspection
*** Limited Object Hierarchy
*** Composition
*** Sensible Defaults
*** Basics of the API
**** Choose a class of model by importing the appropriate estimator class from Scikit-Learn.
**** Choose model hyperparameters by instantiating this class with desired values.
**** Arrange data into a features matrix and target vector following the discussion above.
**** Fit the model to your data by calling the "fit()" method of the model instance.
**** Apply the model to new data:
**** For supervised learning, often we predict lables for the unknown data using the "predict()" method.
**** For unsupervised learning, we often transform or infer properties of the data using the "transform()" or "predict()" method.
*** Supervised learning example: Simple linear regression
   Fitting a line to (x,y) data. (To edit src C-c ')

   #+BEGIN_SRC python
     import matplotlib.pyplot as plt
     import numpy as np
     from sklearn.linear_model import LinearRegression

     # Generate data
     rng = np.random.RandomState(42)
     x = 10 * rng.rand(50)
     y = 2 * x - 1 + rng.randn(50)
     plt.scatter(x, y)

     # Choose a class of model (LinearRegression)
     # Choose model hyperparameters
     model = LinearRegression(fit_intercept=True)
     # Arrange data into a features matrix and target vector
     X = x[:, np.newaxis]
     print(X.shape)
     # Fit the model to your data
     model.fit(X, y)
     print(model.coef_)
     print(model.intercept_)
     # Predict labels for unknown data
     xfit = np.linspace(-1, 11)
     Xfit = xfit[:, np.newaxis]
     yfit = model.predict(Xfit)
   #+END_SRC
*** Supervised learning example: Iris classification
   We will use Gaussian naive Bayes (GnB).
   Fast with no hyperparameters to choose, GnB is often a good model to use as a baseline classification.

   #+BEGIN_SRC python
     from sklearn.cross_validation import train_test_split
     from sklearn.naive_bayes import GaussianNB
     from sklearn.metrics import accuarcy_score

     # Generate training set and testing set
     Xtrain, Xtest, ytrain, ytest = train_test_split(X_iris, y_iris, random_state=1)
     # Follow Basics of the API steps
     model = GaussianNB()
     model.fit(Xtrain, ytrain)
     y_model = model.predict(Xtest)
     print(accuarcy_score(ytest, y_model))
   #+END_SRC
*** Unsupervised learning example: Iris dimensionality
   Reduce the dimensionality of the Iris data.
   The task of dimensionality reduction is to ask whether there is a suitable
   lower-dimensional representation that retains the essential features of the data.
   We will use Principal Component Analysis, which is a fast linear dimensionality
   reduction technique.

   #+BEGIN_SRC python
     from sklearn.decomposition import PCA
     import seaborn as sns

     model = PCA(n_components=2)
     model.fit(X_iris)
     X_2D = model.transform(X_iris)
     iris['PCA1'] = X_2D[:, 0]
     iris['PCA2'] = X_2D[:, 1]
     sns.lmplot("PCA1", "PCA2", hue='species', data=iris, fit_reg=False)
   #+END_SRC
*** Unsupervised learning: Iris clustering
   We will use a powerful clustering method called a Gaussian mixture model (GMM).
   A GMM attempts to model the data as a collection of Gaussian blobs.

   #+BEGIN_SRC python
     from sklearn.mixture import GMM
     import seaborn as sns

     model = GMM(n_components=3, covariance_type='full')
     model.fit(X_iris)
     y_gmm = model.predict(X_iris)
     iris['cluster'] = y_gmm
     sns.lmplot("PCA1", "PCA2", data=iris, hue='species', col='cluster', fit_reg=False)
   #+END_SRC
** Application: Exploring Hand-written Digits
*** Loading and visualizing the digits data
    #+BEGIN_SRC python
      from sklearn.datasets import load_digits
      import matplotlib.pyplot as plt

      # Load dataset
      digits = load_digits()
      print(digits.image.shape)
      # Visualize data
      fig, axes = plt.subplots(10, 10, figsize=(8, 8),
                               subplot_kw={'xticks':[], 'yticks':[]},
                               gridspec_kw=dict(hspace=0.1, wspace=0.1))

      for i, ax in enumerate(axes.flat):
          ax.imshow(digits.images[i], cmap='binary', interpolation='nearest')
          ax.text(0.05, 0.05, str(digits.target[i]),
                  transform=ax.transAxes, color='green')

      X = digits.data
      print(X.shape)

      y = digits.target
      print(y.shape)
    #+END_SRC
*** Classification on digits
    #+BEGIN_SRC python
      from sklearn.naive_bayes import GaussianNB
      from sklearn.metrics import accuracy_score
      from sklearn.metrics import confusion_matrix
      import seaborn as sns

      Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, random_state=0)
      model = GaussianNB()
      model.fit(Xtrain, ytrain)
      y_model = model.predict(Xtest)
      print(accuracy_score(ytest, y_model))

      # Where is the data wrong
      mat = confusion_matrix(ytest, y_model)
      sns.heatmap(mat, square=True, annot=True, cbar=False)
      plt.xlabel('predict value')
      plt.ylabel('true value')

      # Print with predicted labels
      fig, axes = plt.subplots(10, 10, figsize=(8,8),
                               subplot_kw={'xticks':[], 'yticks':[]},
                               gridspec_kw=dict(hspace=0.1, wspace=0.1))
      test_images = Xtest.reshape(-1, 8, 8)

      for i, ax in enumerate(axes.flat):
          ax.imshow(test_images[i], cmap='binary', interpolation='nearest')
          ax.text(0.05, 0.05, str(y_model[i]),
                  transform=ax.transAxes,
                  color='green' if (ytest[i] == y_model[i]) else 'red')
    #+END_SRC
* Hyperparameters and Model Validation
** Thinking abount model validation
*** Model validation the wrong way
    #+BEGIN_SRC python
      from sklearn.datasets import load_iris
      from sklearn.neighbors import KNeighborsClassifier
      from sklearn.metrics import accuracy_score

      iris = load_iris()
      X = iris.data
      y = iris.target
      model = KNeighborsClassifier(n_neighbors=1)
      model.fit(X,y)
      y_model = model.predict(X)
      print(accuracy_score(y, y_model))
    #+END_SRC
*** Model validation the right way: Holdout sets
    A better sense of a model's performance can be found using what's known as a holdout set:
    that is, we hold back some subset of the data form the training of the model, and then use
    this holdout set to check the model performance.
    
    #+BEGIN_SRC python
      from sklearn.cross_validation import train_test_split

      # Split the data with 50% in each set.
      X1, X2, y1, y2 = train_test_split(X, y, random_state=0, train_size=0.5)
      # Fit the model on one set of data
      model.fit(X1, y1)
      # Evaluate the model on the second set of data
      y2_model = model.predict(X2)
      print(accuracy_score(y2, y2_model))
    #+END_SRC
*** Model validation via cross-validation
    Do a sequence of fits where each subset of the data is used both as a training set
    and as a validation set.
    #+BEGIN_SRC python
      y2_model = model.fit(X1, y1).predict(X2)
      y1_model = model.fit(X2, y2).predict(X1)
      print(accuracy_score(y1, y1_model))
      print(accuracy_score(y2, y2_model))
    #+END_SRC
    
    Do a cross validation to use all test data.
    #+BEGIN_SRC python
      from sklearn.cross_validation import cross_val_score
      print(cross_val_score(model, X, y, cv=5))

      from sklearn.cross_validation import LeaveOneOut
      scores = cross_val_score(model, X, y, cv=LeaveOneOut(len(X)))
      print(scores.mean())
    #+END_SRC
** Selecting the best model
   If our estimator is underperforming, how should we move forward?
**** Use a more complicated / more flexible model
**** Use a less complicated / less flexible model
**** Gather more training samples
**** Gather more data to add features to each sample
*** The Bias-variance trade-off
    Underfit the data (high bias).
    Overfit the data (high variance).
*** Validation curves in Scikit-Learn
    #+BEGIN_SRC python
      from sklearn.preproccesing import PolynomialFeatures
      from sklearn.linear_model import LinearRegression
      from sklearn.pipeline import make_pipeline
      import numpy as np

      def PolynomialRegression(degree=2, **kwargs):
          return make_pipeline(PolynomialFeatures(degree),
                               LinearRegression(**kwargs))

      def make_data(N, err=1.0, rseed=1):
          # randomly sample the data
          rng = np.random.RandomState(rseed)
          X = rng.rand(N, 1) ** 2
          y = 10 - 1. / (X.ravel() + 0.1)
          if err > 0:
              y += err * rng.randn(N)
          return X, y

      X, y = make_data(40)
    #+END_SRC
** Learning curves
   One important aspect of model complexity is that the optimal model will generally depend on the size
   of your training data.
**** A model of a given complexity will overfit a small dataset. (High training score, low validation score).
**** A model of a given complexity will underfit a large dataset. (High validation score, low training score).
**** A model will never give a better score to the validation set than the training set.
*** Learning curves in Scikit-Learn
    This is a valuable diagnostic, because it gives us a visual depiction of how our model responds
    to increasing training data.
    In particular, when your learning curve has already converged, adding more training data will not
    significantly improve the fit!
    The only way to incresae the converged score is to use a different (usually more complicated)
    model.
** Validation in Practice: Grid Search
   Here is an example of using grid search to find the optimal polynomial model. We will explore
   a 3-dimensional grid of model features; namely the polynomial degree, the flag telling us whether
   to fit the intercept, and the flag telling us whether to normalize the problem.
   #+BEGIN_SRC python
     from sklearn.grid_search import GridSearchCV

     param_grid = {'polynomialfeatures__degree': np.arange(21),
                   'linearregression__fit_intercept': [True, False],
                   'linearregression__normalize': [True, False]}

     grid = GridSearchCV(PolynomialRegression(), param_grid, cv=7)

     grid.fit(X,y)

     print(grid.best_params_)
   #+END_SRC
* Feature Engineering
** Categorical Features
   One proven technique is to use one-hot encoding, which effectively creates extra columns indicating
   the presence or absence of a category with a value of 1 or 0.
   When your data comes as a list of dictionaries, Scikit-Learn's DictVectorizer will do this for you.
** Text Features
   One of the simplest methods of encoding data is by word counts.
   #+BEGIN_SRC python
     from sklearn.feature_extraction.text import CountVectorizer


     sample = ['problem of evil',
               'evil queen',
               'horizon problem']

     vec = CountVectorizer()
     X = vec.fit_transform(sample)
   #+END_SRC
   Another approach is known as term frequency-inverse document frequency (TF-IDF) which weights
   the word counts by a measure of how often they appear in the documents.
   #+BEGIN_SRC python
     from sklearn.feature_extraction.text import TfidfVectorizer
     import pandas as pd


     sample = ['problem of evil',
               'evil queen',
               'horizon problem']

     vec = TfidfVectorizer()
     X = vec.fit_transform(sample)
     pd.DataFrame(X.toarray(), columns=vec.get_feature_names())
   #+END_SRC
** Image Features
   Scikit-Image project.
** Derived Features
** Imputation of Missing Data
   Often, the NaN value is used to mark missing values.
   When applying a typical machine learning model to such data, we will need to first replace such missing
   data with some appropiate fill value. This is known as imputation of missing values.
   #+BEGIN_SRC python
     from sklearn.preprocessing import Imputer

     imp = Imputer(strategy='mean')
     X2 = imp.fit_transform(X)
   #+END_SRC
** Feature Pipelines
   We might want a proccesing pipeline that looks something like this:
**** Impute missing values using the mean.
**** Transform features to quadratic.
**** Fit a linear regression.
   #+BEGIN_SRC python
     from sklearn.pipeline import make_pipeline

     model = make_pipeline(Imputer(strategy='mean'),
                           PolynomialFeatures(degree=2),
                           LinearRegression())
   #+END_SRC
* In Depth: Naive Bayes Classifications
